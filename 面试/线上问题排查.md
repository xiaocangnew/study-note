### [线上问题排查](https://blog.csdn.net/weixin_34162228/article/details/93019589)
0. 问题描述
    1. 线上的服务突然变慢(通常是gc问题)
    2. 突然out-of-memeory
    3. cpu利用率变高
    4. 磁盘io占用过高(大量读写文件)
    5. 网络延迟变长
1. 业务相关
    1. 查看日志中Error错误，统计error错误数量。
    2. PV量过高
         (日志/监控)分析服务请求量，耗时，成功率和失败率
    3. 服务调用耗时异常
         耗时过长的服务调用如果没有熔断等机制，很容易导致应用性能下降或服务不可用，服务不可用很容易导致雪崩。
    4. 死锁，多线程并发
    5. 异常安全攻击扫描等。
2. 数据库相关
    一条sql没写好导致慢查询，可能会导致整个应用挂起
    此时需要查看数据库连接请求、是否连接数过大，是否出现死锁、查看数据库慢日志定位具体SQL
3. JVM相关
      3.1 OOM相关
           发生OOM问题一般服务都会crash，业务日志会有OutOfMemoryError
      3.2 死锁
           死锁原因是两个或者多个线程相互等待资源。现象通常是出现线程hung住。
           更严重会出现线程数暴涨，系统出现api alive报警等。查看死锁最好的方法就是分析当时的线程栈。
            jstack -l pid (打印进程的栈信息，长列表打印，关于锁的附加信息，同时会使jvm停顿很久)
      3.3 线程block、线程数暴涨
           1. 线程block问题通常是等待io、等待网络、等待监视器锁等造成，
               可能会导致请求超时、造成造成线程数暴涨导致系统502等。
           2. 使用命令：
               jstack -l pid |wc -l
               jstack -l pid |grep "BLOCKED"|wc -l
               jstack -l pid |grep "Waiting on condition"|wc -l
      3.4 [gc时间过长](https://blog.csdn.net/goldenfish1919/article/details/97155089)
             使用jstat命令查看gc时间，如果过长，需要调优
             1. 对象创建的速度过高，随之而来的就是GC频率也会变快，然后会导致GC的停顿时间变长。
                 把GC日志上传到gceasy.io，这个工具会告诉你对象的创建速度
             2. Young区过小
             3. 选择合适的GC算法, 使用g1垃圾回收器，23点都没有了
             4. 进程被交换（Swap）出内存, 也会导致gc时间变长
             5. GC线程数过少
             6. IO负载重
4. Server本身问题
     1. CPU占用率过高,CPU上下文切换频率次数较高
          1. 通过top命令查看，
               loadAvge < cpu*0.7
               wa 表示io等待时间占比；
          2. vmstat查看：
               cs (context switch) 每秒的上下文切换次数
               in (interrupt) 每秒的中断次数
               r (Runing or Runnable) 就绪队列的长度，也就是正在运行和等待CPU的进程数。
               b (Blocked) 不可中断睡眠状态的进程数
          3. pidstat查看进程的上下文交换
                cswch 自愿上下文切换，指的是进程无法获得所需的资源导致的上下文切换，比如I/O不足，内存不足
                nvcswch 非自愿上下文切换，指的是 进程由于时间片已到等原因，被系统强制调度，进而发生上下文切换。比如大量进程在争抢CPU
          4. 一般上下文切换在数百到一万之内。上下文切换超过1万，很可能遇到性能问题。
     2. 磁盘满了, 磁盘I/O过于频繁
          通过df查看磁盘空间， iostat查看磁盘io情况
     3. 网络流量异常 (连接数过多, 网络延迟变长)
          通过netstat命令查看网络流量 (查看协议，ip:port等)
          [问题排查](../网络/http-socket/tcp问题记录.md)
     4. 系统可用内存长期处于较低值 (导致 oom killer) 等等。
          通过free命令查看内存空间，vmstat查看内存情况
     5. 关注网络连接：
           查看tcp各个状态数量：netstat -an | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'
           查看连接某服务端口最多的IP：netstat -na | grep 172.16.70.60:1111 | awk '{print $5}' | cut -d : -f1 | sort | uniq -c | sort -rn | head -10

 ### 某一个接口间隔性的出现耗时较长，如何排查
 1. 如果所有接口都是间隔一段时间耗时长一点，那么可能是gc导致的，看gc
 2. 如果只有一个接口是这样，那么需要进行压测该接口，把偶发性问题变成一直出现的，然后看线程日志，定位代码问题

 ### 一次oom线上问题解决
 - 前提： jar包启动时配置了参数 XX:+HeapDumpOnOutOfMemoryError
 0. 根据关键词 “java.lang.OutOfMemoryError”进行搜索日志(根据提示看代码，如果找到最好，否则下一步)
 1. 使用MAT打开拿到的hprof文件进行分析。
 2. 打开Histogram看看占用内存最大的是什么对象：
     2.1. 在最大对象上右键：merge shortest path to gcroots --> with all references
     2.2. 点开对象的内容是什么(有可能是内容导致的)
     2.3. (名词解释： shallow heap 是对象本身大小， retained heap为对象和他的引用总共大小)
 5. 分析业务代码
 6. [网页来源](https://www.cnblogs.com/lovecindywang/p/10800593.html)
 修改springboot配置
   server:
    tomcat:
     max-http-header-size: 65536
 7. [url异常](https://blog.csdn.net/gallenzhang/article/details/98520496)

### 一次测试环境 cpu 100% 问题排查
 - 某个业务代码导致(代码没有严格review导致某处while死循环)
 1. 使用top命令查看cpu高的程序
 2. 使用 top -H -p PID 查看程序中cpu高的线程
 3. 线程id进制转换 printf “%x\n“ ppid
 4. jstack -l ppid

 ### minor gc运行的很慢有可能是什么原因引起的？
 1、 新生代空间设置过大。
 2、 对象引用链较长，进行可达性分析时间较长。
 3、 新生代survivor区设置的比较小，清理后剩余的对象不能装进去需要移动到老年代，造成移动开销。
 4、 内存分配担保失败，由minor gc转化为full gc
 5、 采用的垃圾收集器效率较低，比如新生代使用serial收集器(正常使用多线程版本paraNew)

 ### 频繁full gc导致
 0. 通过命令查看cpu占用(和业务代码导致cpu过高一致)
 1. 每个线程资源占用分布平均，查看当前进程的GC问题
 2. 观看每秒一次的GC状态 #jstat -gcutil 10010 1000 10#
 3. S0,S1区，E区这2个区一般不会有什么问题，除非配置极端不合理
    O区(老年代)资源占用率过高导致FGC，处理方式是进行资源调整或者检查程序是否有什么异常
    M区(永久代)资源占用率过高，
     查看是否有过多的class加载#jstat -class 10010# 或者是否有大的对象无法GC#jmap -histo 10010#