### jvm 经常性抖动，但未超时，是什么原因呢，(堆内存长期处于高位但未崩溃的现象，核心原因及详细分析)
0. 业务逻辑导致的内存波动
   批处理任务：短时间内处理大量数据，导致堆内存激增。或者大对象处理导致。
   缓存未淘汰：缓存策略未设置LRU或TTL，内存无限增长。
   错误代码： 代码中出现不正常的内存使用，比如List 错误使用，申请大量内存，或者死循环等。
1. 内存泄漏（最常见原因）
   现象：老年代内存持续增长，即使触发Full GC也无法显著下降。
   典型表现：
   老年代使用率长期高于80%，且每次Full GC后回收的内存量极少。
   堆转储（Heap Dump）中发现大量重复对象或无效引用（如静态集合、未关闭的资源）。
   常见场景：
   静态集合类：如static Map长期持有对象引用。
   未释放资源：数据库连接、文件流、网络连接未正确关闭。
   监听器/回调：注册后未注销（如Spring事件监听器）。
   缓存策略：缓存未设置过期时间或淘汰机制（如Guava Cache未配置expireAfterWrite）。
2. 对象存活时间过长，频繁晋升老年代
   现象：年轻代对象快速晋升到老年代，导致老年代空间不足。
   原因：
   年轻代过小：-Xmn设置过小，导致对象在Young GC后存活的对象直接进入老年代。
   大对象直接进入老年代：如一次性加载大文件或序列化数据。
   验证方法：
   使用jstat -gc <pid>观察FGC（Full GC次数）和OU（老年代使用率）。
   检查年轻代对象晋升率（jstat -class <pid>中的YGC和FGC比例）。
3. 内存碎片化（CMS收集器常见问题）
   现象：老年代有足够空闲内存，但因碎片化无法分配大对象。
   触发条件：使用CMS收集器（-XX:+UseConcMarkSweepGC）时，频繁Full GC后产生碎片。
   验证方法：
   检查GC日志中是否有promotion failed或concurrent mode failure。
   使用jmap -histo:live <pid>观察老年代对象分布。
4. JVM参数配置不当
   堆大小不合理：
   -Xmx设置过小，无法满足业务需求。
   -Xms与-Xmx不一致，导致堆动态调整开销。
   Survivor区比例失衡：
   -XX:SurvivorRatio设置过小（如默认8:1:1），导致Eden区过小，对象过早晋升。


### 抖动都有哪些影响呢
1. 应用性能下降
     GC抖动会导致应用线程暂停。频繁的GC会导致应用响应时间增加，影响用户体验。比如高并发场景下，每次 Full GC 停顿 2 秒，可能导致 1000 个并发请求超时。
2. GC 频繁占用 CPU 资源，导致有效计算时间减少，吞吐量降低
3. 资源竞争加剧， 导致其他线程的执行效率下降， 关键任务五福啊及时处理(支付回调)


#### 如何监控并发现抖动问题呢
1. 对于oom 发生后的内存堆栈， 可以通过 jmap -dump:live,format=b,file=heap.bin <pid> 进行dump
2. 对于未发生oom的内存堆栈，分析时，可以写shell脚本，jstat命令实时监控占用的堆栈大小，超过阈值就手动 jmap。
3. 也可以人工降低一台机器的内存，将抖动升级为oom。

````
# 获取老年代大小（MB）
get_old_gen_size() {
    # jstat -gc <PID> 输出示例:
    # S0C    S1C    S0U    S1U      EC       EU        OC         OU       ...
    # 2048.0 2048.0  0.0    0.0   3072.0   1024.0   20480.0     5120.0   ...
    local oc
    oc=$(jstat -gc "$PID" | awk 'NR==2 {print $8}')  # OC: Old Used
    if [ -z "$oc" ]; then
        echo "$(date '+%Y-%m-%d %H:%M:%S') 获取老年代容量失败。" | tee -a "$LOG_FILE"
        exit 1
    fi
    # 转换为整数 MB
    echo "$oc" | awk '{printf "%.0f",$1/1024}'
}
````