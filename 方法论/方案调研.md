1、需求
以下数据均依赖一年内的pipe数据
共67个基础标签，
总用户数为2.4亿
总用户增长为5kw
日活总量为1亿
日活人次为 6.4kw
日活峰值为5百万
用户活跃时间为8:00到24:00
活跃用户获取tag tps:假设分别查询每一个tag，5,448,451 * 67 / (24 - 8) / 60 / 60 = 6337
插入&删除 tps:
    假设4个小时完成导入（每个tag一条命令）： 365,046,217 / 4 / 60 / 60 = 25350
    假设4个小时完成导入（67个tag一条命令）： 5,448,451 / 4 / 60 / 60 = 379

需要支持的操作：
（1）根据(userId，tag)的读
（2）批量（userId，tag）读
（3）根据userId查询所有tag
（4）根据userId批量添加tag
（5）根据userId批量修改tag
（6）根据tag和tag下value查询userId


solar-push rpc qps 峰值： 20
leo-push rpc qps 峰值：0.15 (表上有一个qps 38的时间段，当做非正常情况处理，并没有纳入这里的统计)

2、基本方案
选择竖表 + 搜索引擎的方式，搜索引擎难以满足(4)，(5)操作的qps要求，竖表不能满足(6)操作。

(6)操作主要是后台，查询量不高，但也要求快速响应，实时性要求低于前5个，

数据流： Hive → 竖表 → 搜索引擎

竖表要求： 容量方便拓展
操作(1),(2)查询qps至少2w
操作(4),(5) tps 至少1k

搜索引擎要求： 容量方便拓展
支持对tag下value进行预处理，以及并根据处理结果查询
操作(6) tps至少40，响应时间5s以内

3、竖表
3.1、数据结构
id	int	记录主键，可能是ytkId，solarId，deviceId等
tag	string	标签名
value	string	标签值
updatedTime	int	更新时间，单位秒
3.2、mysql
熟悉的mysql并不能胜任，拓展性比较差

3.3、HBase
HBase可以满足竖表的要求。

同时阿里云还提供了https://help.aliyun.com/document_detail/88404.html?spm=a2c4g.11174283.6.579.22be3c2eV0T2cH，hbase + solr 的方案，提供全文搜索能力，但文档中说明了最好只把需要查询的字段添加到solr索引中，索引多了会对性能影响比较大。

3.4、MongoDb
MongoDb支持集群，使用Hashed Shard Key进行分片，可以加副本集保证可用性。
单独更新一个字段相当于整体文档，考虑到hive数据是全量的，一起导入的话问题不大。
mongo不能很好满足(6)操作，还是需要一个搜索引擎来处理
阿里云MongoDb性能报告，https://help.aliyun.com/document_detail/73624.html?spm=a2c4g.11186623.6.685.f4e79bab7VDA0t

性能可以保证。

可以考虑把标签提出做一张新表，不常修改的标签做一个表，常修改的做一个

es提供了mongoDb作为数据源的插件，导数据比较方便。

3.5、TableStore
与HBase类似，都是BigTable的实现，支持索引，和rest api，综合来看比Hbase好用些。
同时阿里云提供了https://help.aliyun.com/document_detail/62137.html?spm=a2c4g.11186623.6.589.3ae56575hTXHPi TableStore 到 es 的数据通道。可以减少一些开发和维护成本

3.6、HybridDB for MySQL
https://www.aliyun.com/product/petadata?spm=5176.8142029.cloudEssentials.46.e9396d3eHdOqqr

分布式架构 + Mysql兼容，好处就是比较熟悉。缺点就是必须把数据做出竖表的样子，批量取用户数据费点儿劲；每个节点的数据容量没法调，只能是512GB，有点太大了，经济上不是很合适

4、搜索引擎
4.1、es vs solr
https://logz.io/blog/solr-vs-elasticsearch/

选择es的原因：
（1）团队内es应用场景多一些，方便解决各种问题
（2）es的数据分析能力比solr强，solr更倾向于全文搜索
（3）es在索引时性能更好

5、计费
考虑四种方案：

（1）HBase + Solr
（2）TableStore + Es
（3）MongoDb + Es
（4）HBase + Es

5.1、数据大小预估
预估一个用户67个tag的大小：

id为solarId或ytkId均为int，4字节

67个tag的value中，枚举类的有6个，数字类的有54个，字符串类的有7个
枚举类视为int，4字节；
数字类视为int或float，4字节
字符串类假设平均20个UTF-8字符，40字节。
updatedTime为int ，4字节；
一行用户数据需要 4 + 4 +（6 * 4 + 54 * 4 + 7 * 40 ） =  582字节
总用户的数据量，ytkId和solarId分别对应一行数据： 241,993,184 * 2 * 582 = 281,680,066,176 字节(280GB，未压缩)
每年增长：55,156,481 * 2 * 582 = 64,202,143,884 字节(64GB，未压缩)

考虑到今后拓展，未压缩数据暂定400GB

5.2、HBase + Solr
hbase 提供了GZIP，LZO，Snappy三种压缩方式，Snappy压缩和解压速度比较快，可以压缩约22%。
压缩后数据约为 400  * （1 - 22%） = 312 GB

core单节点容量，是目标容量的两倍是因为云盘系列集群采用双副本架构，实际可用存储容量为总容量的一半。
考虑到需要solr服务，core和规格和节点容量都比推荐的高一些。

参考：https://help.aliyun.com/document_detail/59012.html?spm=a2c4g.11186623.6.551.1a362c4cJqki04

5.3、TableStore + Es
TableStore根据四个指标收费，

（1）数据存储量
（2）预留读/写吞吐量
（3）按量读/写吞吐量
（4）外网下行流量

5.3.1、数据存储量
String	UTF-8 字符串占用的字节数（表格存储允许值为空的 String 类型，如果字符串为空，则数据大小为 0。）
Integer	8
Double	8
Boolean	1
Binary	二进制数据占用的字节数
重新计算每行的数据大小：8 + 8 +（6 * 8 + 54 * 8 + 7 * 40 ）= 776 字节

总数据量：241,993,184 * 2 * 776 = 370,733,557,888 字节 （370GB）

5.3.2、预留读/写吞吐量 和 按量读/写吞吐量
读吞吐量：

单行读操作返回数据大小大于4KB，则消耗读CU = 实际数据大小按4KB整除向上取整。
对于BatchGet的多行读操作，消耗的读CU为所有单行读操作消耗的CU之和。每单行读操作消耗的读CU按规则1对该行进行收费。
如果是 GetRange的批量读操作，则消耗读CU = 读取到的所有行主键的数据大小与每行实际读取的属性列数据大小之和除以4KB向上取整。
读操作的4xx错误会消 耗1个读CU。
写操作的4xx错误会消耗1个写CU，如果是指定行存在性检查条件不满足而写失败，则会额外消耗1个读CU。
5xx错误不消耗CU。

写吞吐量：
计量规则与读操作一致



预留读吞吐量：
以每秒1次4KB单行读操作为1个预留读CU（ Capacity Unit，能力单元）。
设置预留读吞吐量之后，实际消耗没有达到预留值也会按照预留值进行收费。

预留写吞吐量：
以每秒1次4KB单行写操作为1个预留写CU（Capacity Unit，能力单元）。
设置预留写吞吐量之后，实际消耗没有达到预留值也会按照预留值进行收费。



依据：https://help.aliyun.com/document_detail/72660.html?spm=a2c4g.11186623.6.551.59ad73bfkRQqQ9



6、结论
HBase + Solr，TableStore + Es，HBase + Es 都是可选方案；

HybridDB for MySQL的问题在于可选容量太大。

MongoDb的问题在于IOPS跟HBase和TableStore比差距还是比较大，而且容量比较贵。

因为HBase + Solr 和 TableStore + Es 都提供了数据导入的办法，可以省去自己写导入脚本的事情，更倾向于这两种。

HBase Vs TableStore:

https://yq.aliyun.com/articles/69547

因为对于竖表的操作比较简单，复杂操作都交给搜索引擎处理了，所以HBase和TableStore的差异并不大。