### zab协议
- ZXID通常由64位组成。
    高32位表示Epoch值，翻译过来指朝代。就是每次换了Leader之后，Epoch值都会增加1
    低32位表示事务值，每次加1。当更换leader后，置为0；
- 每个节点的状态：       
    election/leading/fellowing
    选举刚刚开始的时候，每个节点在没有先验信息的情况下都把选票投向自己，并把这个消息发送给所有的节点，然后等待其他节点们的响应，
- 选举轮次
   每一轮的选取会有一个递增的round number作为标识，这个值越高优先级越高；
- FLE算法(fastLeaderElecttion)
   1. 筛选具有最大LastZxid(history中最后一个提案的ZXID编号)的节点作为候选Leader
   2. 每个节点都只能投一张选票，只有这样才能确定过半选票的统计值
   3. 节点更新自己选票的标准就是具有更新的提案号：要么具有更新的epoch，或者在相同epoch下具有更大的机器编号。
   
- zab协议由4个阶段组成
一. leader选举阶段(leader轮次)
 节点收到其他节点的选举信息(zxid)的时候：
1. 看round number，比自己旧忽略，比自己新则更新自己的round number，并清空上一轮相关的陈旧信息开始广播自己新的选票；
2. 如果是在同一轮投票中：
     如果收到的选票角色是election，并且该消息附带更新的提案号，则更新自己的选票并继续广播自己的选票；
     如果收到的选票角色是election，但是消息的提案号比自己旧或者跟自己一样，则记录这张选票，
       而检查发现自己得到针对某个节点超过集群半数的选票，自己切换为leading/fellowing状态，并转入Phase Recovery；
3. 任何时候一旦收到leading/fellowing的选票，都指明当前集群中已有有效的候选Leader了，直接更新自己切换入Phase Recovery阶段；

二. Phase Recovery阶段 (日志选举轮次，准leader发现全局最新事务)
这个阶段选出的准Leader(prospective leader)肯定是集群中过半数机器的投票选出的leader。
   1. 统一epoch(follower-->leader-->follower-->leader)
       1. 所有节点会把自己的acceptedEpoch通过(followerInfo信息)发送给准leader，
       2. 当准Leader得到过半的(followerInfo信息)时，会在收到消息中取出所见最大的epoch并将其递增，这样之前的Leader就不能再提交新的提案了，
       3. 然后准Leader再将这个新epoch通过(newepoch信息)发回给这些节点并等待确认。
       4. 在Fellower节点收到候选Leader发送(newepoch信息)后，将其与自己本地的acceptedEpoch对比，
         如果比他们大就更新自己acceptedEpoch，并返回(ack newepoch信息)后进入同步阶段，否则切换会leader选举状态。
   2. 准Leader在收到过半数目的(ack newepoch信息)会进入同步阶段。
         Fellower发送的(ack newepoch信息)包含了额外的重要信息——自己最新提交日志，
         这样候选Leader在收集ACKEPOCH的同时就知道哪个Fellower具有最新提交了，选定到这个具有最新提交的Fellower后向其同步日志.

三. 各个follower同步最新事务 (leader-->follower-->leader-->follower)
    1. 准Leader已经确立了最新任期号和最新提交日志，然后他会把自己的history通过(newleader信息)发送给所有的集群成员，
    2. 集群成员更新自己currentEpoch并按需同步history信息。
    3. Fellower向Leader发送(ackNewLeader消息)，
    4. 准Leader得到过半数目的(ackNewLeader消息)后，会向所有的Fellower发送COMMIT并进入广播流程，
       而Fellower接收到COMMIT命令而完成提交后，也会切换到广播流程。

四. 正常工作流程(广播)
   1.改变自身状态到leader或follower,一致性协议保证此时只可能会有一个Leader。
   2.接受新消息时使用2PC(与标准的2PC有区别，只需要半数同意)操作。
     其中2PC是指参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。


### raft协议
- 基本概念
  - 复制状态机(Raft协议可以使得一个集群的服务器组成复制状态机)
        1. 一个分布式的复制状态机系统由多个复制状态机组成；
        2. 复制状态机的状态保存在一组状态变量中，状态机的变量只能通过外部命令来改变
        3. 每一个状态机存储一个包含一系列指令的日志，严格按照顺序逐条执行日志中的指令，如果所有的状态机都能按照相同的日志执行指令，
         那么它们最终将达到相同的状态。因此，在复制状态机模型下，只要保证了操作日志的一致性，我们就能保证该分布式系统状态的一致性。
   - 每个节点的状态
        follower(启动初始化时)-->candidate-->leader
   - time out信号 
       集群刚启动时，所有节点都是follower，之后在time out信号的驱使下，follower会转变成candidate去拉取选票，
       获得大多数选票后就会成为leader，这时候如果其他候选人发现了新的leader已经诞生，就会自动转变为follower；
       而如果另一个time out信号发出时，还没有选举出leader，将会重新开始一次新的选举。
       可见，time out信号是促使角色转换得关键因素，类似于操作系统中得中断信号。
   - term
       1.在Raft协议中，将时间分成了一些任意长度的时间片，称为term，term使用连续递增的编号的进行识别。
       2.每一个term都从新的选举开始：
         candidate们会努力争取称为leader。一旦获胜，它就会在剩余的term时间内保持leader状态。
         在某些情况下选票可能被多个candidate瓜分，形不成多数派，因此term可能直至结束都没有leader，下一个term很快就会到来重新发起选举。
       3.server的term编号
        在server之间进行交流的时候就会带有该编号，它会将自己的编号更新为较大的那一个；
        如果leader和candidate发现自己的编号不是最新的了，就会自动转变为follower；
        如果接收到的请求的term编号小于自己的当前term将会拒绝执行。  
   - rpc (server之间的交流是通过RPC进行的。只需要实现两种RPC就能构建一个基本的Raft集群)
       1.RequestVote RPC：它由选举过程中的candidate发起，用于拉取选票
       2.AppendEntries RPC：它由leader发起，用于复制日志或者发送心跳信号。


- leader选举过程。 (Raft通过心跳机制发起leader选举)
1. 节点都是从follower状态开始的.
    如果收到了来自leader或candidate的RPC，那它就保持follower状态，避免争抢成为candidate。Leader会发送空的AppendEntries RPC作为心跳信号来确立自己的地位.
    如果follower一段时间(election timeout)没有收到心跳，它就会认为leader已经挂了，发起新的一轮选举。
2. 选举发起后，一个follower会增加自己的当前term编号并转变为candidate。它会首先投自己一票，之后candidate状态将可能发生如下三种变化:
  2.1 赢得选举,成为leader: 
        如果它在一个term内收到了大多数的选票，将会在接下的剩余term时间内称为leader，然后就可以通过发送心跳确立自己的地位。
  2.2 其他server成为leader：
       在等待投票时，可能会收到其他server发出AppendEntries RPC心跳信号，说明其他leader已经产生了。这时通过比较自己的term编号和RPC过来的term编号:
        1.如果比对方大，说明leader的term过期了，就会拒绝该RPC,并继续保持候选人身份; 
        2.如果对方编号不比自己小,则承认对方的地位,转为follower.
  2.3 选票被瓜分,选举失败: 
      如果没有candidate获取大多数选票, 则没有leader产生, candidate们等待超时后发起另一轮选举.
      raft采用随机election timeout的机制防止选票被持续瓜分。通过将timeout随机设为一段区间上的某个值, 因此很大概率会有某个candidate率先超时然后赢得大部分选票.


- 日志复制同步过程 (选出leader后，两阶段提交过程)
1. 客户端提交每一条命令都会被按顺序记录到leader的日志中，每一条命令都包含term编号和顺序索引，
2. 然后向其他节点并行发送AppendEntries RPC用以复制命令(如果命令丢失会不断重发)，
3. 当复制成功也就是大多数节点成功复制后，leader就会提交命令(执行该命令),并且返回客户端，
*. raft保证已经提交的命令最终也会被其他节点成功执行。
*. leader会保存有当前已经提交的最高日志编号。顺序性确保了相同日志索引处的命令是相同的，而且之前的命令也是相同的。
*. 当发送AppendEntries RPC时，会包含leader上一条刚处理过的命令，接收节点如果发现上一条命令不匹配，就会拒绝执行。
*. Leader不是一接收到写请求就发送，而是按照固定的心跳周期单个或批量发送。

- 日志复制过程中的不一种情况
如果leader崩溃了，它所记录的日志没有完全被复制，会造成日志不一致的情况，
follower相比于当前的leader可能会丢失几条日志，也可能会额外多出几条日志，这种情况可能会持续几个term。
   - 解决日志不一致
      在Raft中，leader通过强制follower复制自己的日志来解决上述日志不一致的情形，那么冲突的日志将会被重写。
      如果能和follower的当前索引对上，那就执行，否则拒绝，然后leader将会逐次递减索引，直到找到相同的那条日志。
   - 强制follower复制leader日志的问题
      Raft通过投票过程确保只有拥有全部已提交日志的candidate能成为leader。
           由于candidate为了拉选票需要通过RequestVote RPC联系其他节点，而之前提交的命令至少会存在于其中某一个节点上,
           因此只要candidate的日志至少和其他大部分节点的一样新就可以了, follower如果收到了不如自己新的candidate的RPC,就会将其丢弃.
   - 提交未完成的日志
     如果命令已经被复制到了大部分节点上,但是还没来的及提交就崩溃了,这样后来的leader应该完成之前term未完成的提交.
     Raft通过让leader统计当前term内还未提交的命令已经被复制的数量是否半数以上, 然后进行提交.


     
- 日志压缩
1. Snapshotting是最简单的压缩方法，系统的全部状态会写入一个snapshot保存起来，然后丢弃截止到snapshot时间点之前的所有日志。
2. 当follower严重落后于leader时，leader需要把自己的snapshot发送给follower加快同步，此时用到了一个新的RPC：InstallSnapshot RPC。
   follower收到snapshot时，需要决定如何处理自己的日志
    1. 如果收到的snapshot包含有更新的信息，它将丢弃自己已有的日志，按snapshot更新自己的状态，
    2. 如果收到的snapshot包含的信息更少，那么它会丢弃snapshot中的内容，但是自己之后的内容会保存下来
    
- 成员变更





### 如何解决脑裂
- ZooKeeper和Raft都是过半即可，所以对于分区是容忍的。
1. 如5台机器，分区发生后分成2部分，一部分3台，另一部分2台，这2部分之间无法相互通信
2. 含有3台的那部分，仍然可以凑成一个过半，仍然可以对外提供服务，但是它不允许有server再挂了，一旦再挂一台则就全部不可用了。
3. 含有2台的那部分，则无法提供服务，即只要连接的是这2台机器，都无法执行相关请求。
所以ZooKeeper和Raft在一旦分区发生的情况下是是牺牲了高可用来保证一致性，即CAP理论中的CP。








### [zookeeper VS raft](https://blog.csdn.net/weixin_36145588/article/details/78477159)
1. leader的选举
1.1 一般的leader选举过程
   - 选举的轮次
      1. ZooKeeper有2个轮次，一个是选举轮次electionEpoch，另一个是日志的轮次peerEpoch（即表示这个日志是哪个轮次产生的）
      2. Raft则是只有一个轮次，相当于日志轮次和选举轮次共用了
   - 选举出来的leader要包含更多的日志
      1. 定义最后一个日志记录: 一种就是所有日志中的最后一个日志，另一种就是所有已提交中的最后一个日志。 zookeeper和raft都是第一种。
      2. zookeeper使用最后一个日志记录方式保证了leader包含所有的记录。因为ZooKeeper在每次leader选举完成之后，都会进行数据之间的同步纠正，所以每一个轮次，大家都日志内容都是统一的
      3. Raft在leader选举完成之后没有这个同步过程，而是靠之后的AppendEntries RPC请求的一致性检查来实现纠正过程，则就会出现隔了几个轮次还不统一的现象
1.2. leader选举的效率
   -  Raft
        server在某个term轮次内只能投一次票，哪个candidate先请求投票谁就可能先获得投票，这样就可能造成split vote，
        即各个candidate都没有收到过半的投票，Raft通过candidate设置不同的超时时间，来快速解决这个问题
   -  ZooKeeper
        server某个electionEpoch轮次内，可以投多次票，只要遇到更大的票就更新，然后分发新的投票给所有人。这种情况下不存在split vote现象，
        同时有利于选出含有更新更多的日志的server，但是选举时间相对Raft要花费的多。
1.3 加入一个已经完成选举的集群
    - 怎么发现已完成选举的leader？
       1. Raft比较简单，该server启动后，会收到leader的AppendEntries RPC,这时就会从RPC中获取leader信息，识别到leader
       2. 该server启动后，会向所有的server发送投票通知，这时候就会收到处于LOOKING、FOLLOWING状态的server的投票
加入过程是否对leader处理请求的过程造成阻塞？
1.4 leader选举的触发
  1. server刚开始启动的时候，触发leader选举
  2. leader选举完成之后，检测到超时触发，谁来检测？
     - Raft：只是follower在检测。follower有一个选举时间，在该时间内如果未收到leader的心跳信息，则follower转变成candidate，自增term发起新一轮的投票，leader遇到新的term则自动转变成follower的状态
     - ZooKeeper：leader和follower都有各自的检测超时方式，leader是检测是否过半follower心跳回复了，follower检测leader是否发送心跳了。
2 上一轮次的leader
2.1 上一轮次的leader的残留的数据怎么处理？
    1. 会有已过半复制的日志和未过半复制的日志
    2. 
2.2 怎么阻止之前的leader假死的问题
3 请求处理流程
3.1 请求处理的一般流程
3.2 日志的连续性问题
3.3 如何保证顺序
3.3.1 正常同步过程的顺序
3.3.2 异常过程的顺序
follower挂掉又连接
leader更换
3.4 请求处理流程的异常
4 分区的处理
